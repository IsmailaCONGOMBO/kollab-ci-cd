
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                      ARGS                      ‚îÇ PROFILE  ‚îÇ        USER         ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ                                                ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 18:04 GMT ‚îÇ 03 Nov 25 18:27 GMT ‚îÇ
‚îÇ kubectl ‚îÇ -- get po -A                                   ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 18:29 GMT ‚îÇ 03 Nov 25 18:29 GMT ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                 ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:03 GMT ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                 ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:11 GMT ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                 ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:14 GMT ‚îÇ                     ‚îÇ
‚îÇ image   ‚îÇ build -t kollab-backend:latest ./Kollab        ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:18 GMT ‚îÇ 03 Nov 25 19:18 GMT ‚îÇ
‚îÇ image   ‚îÇ build -t kollab-frontend:latest ./kollab-front ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:18 GMT ‚îÇ 03 Nov 25 19:18 GMT ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                 ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:19 GMT ‚îÇ                     ‚îÇ
‚îÇ stop    ‚îÇ                                                ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:27 GMT ‚îÇ 03 Nov 25 19:27 GMT ‚îÇ
‚îÇ start   ‚îÇ --cpus=4 --memory=8192                         ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:29 GMT ‚îÇ                     ‚îÇ
‚îÇ start   ‚îÇ --cpus=4 --memory=6144                         ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:30 GMT ‚îÇ 03 Nov 25 19:33 GMT ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                 ‚îÇ minikube ‚îÇ ISMAIL-ZERTUY\somma ‚îÇ v1.37.0 ‚îÇ 03 Nov 25 19:34 GMT ‚îÇ 03 Nov 25 19:35 GMT ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/11/03 19:30:48
Running on machine: Ismail-Zertuy
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1103 19:30:48.708234   12428 out.go:360] Setting OutFile to fd 124 ...
I1103 19:30:48.736841   12428 out.go:413] isatty.IsTerminal(124) = true
I1103 19:30:48.736841   12428 out.go:374] Setting ErrFile to fd 128...
I1103 19:30:48.736841   12428 out.go:413] isatty.IsTerminal(128) = true
W1103 19:30:48.751275   12428 root.go:314] Error reading config file at C:\Users\somma\.minikube\config\config.json: open C:\Users\somma\.minikube\config\config.json: Le fichier sp√©cifi√© est introuvable.
I1103 19:30:48.756479   12428 out.go:368] Setting JSON to false
I1103 19:30:48.757611   12428 start.go:130] hostinfo: {"hostname":"Ismail-Zertuy","uptime":3335,"bootTime":1762194913,"procs":271,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26200.6899 Build 26200.6899","kernelVersion":"10.0.26200.6899 Build 26200.6899","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"6ecf910c-6382-43ee-9430-8acef198d94c"}
W1103 19:30:48.757611   12428 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1103 19:30:48.760530   12428 out.go:179] üòÑ  minikube v1.37.0 on Microsoft Windows 11 Pro 10.0.26200.6899 Build 26200.6899
I1103 19:30:48.762359   12428 notify.go:220] Checking for updates...
I1103 19:30:48.762898   12428 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1103 19:30:48.762898   12428 driver.go:421] Setting default libvirt URI to qemu:///system
I1103 19:30:48.870887   12428 docker.go:123] docker version: linux-28.5.1:Docker Desktop 4.49.0 (208700)
I1103 19:30:48.877316   12428 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1103 19:30:49.201614   12428 info.go:266] docker info: {ID:44c6211f-5113-420c-b732-5fef55eff5da Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:9 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:46 OomKillDisable:false NGoroutines:101 SystemTime:2025-11-03 19:30:49.177160031 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8252514304 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Users\somma\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:model Path:C:\Users\somma\.docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-model.exe] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.46] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.1] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.3.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1103 19:30:49.203313   12428 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1103 19:30:49.204838   12428 start.go:304] selected driver: docker
I1103 19:30:49.204838   12428 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1103 19:30:49.204838   12428 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1103 19:30:49.275145   12428 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1103 19:30:49.605701   12428 info.go:266] docker info: {ID:44c6211f-5113-420c-b732-5fef55eff5da Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:9 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:46 OomKillDisable:false NGoroutines:101 SystemTime:2025-11-03 19:30:49.58369091 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8252514304 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Users\somma\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:model Path:C:\Users\somma\.docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-model.exe] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.46] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.1] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.3.1] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
W1103 19:30:49.606243   12428 out.go:285] ‚ùó  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
W1103 19:30:49.606243   12428 out.go:285] ‚ùó  You cannot change the CPUs for an existing minikube cluster. Please first delete the cluster.
I1103 19:30:49.606776   12428 cni.go:84] Creating CNI manager for ""
I1103 19:30:49.609062   12428 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1103 19:30:49.609629   12428 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1103 19:30:49.610708   12428 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1103 19:30:49.612885   12428 cache.go:123] Beginning downloading kic base image for docker with docker
I1103 19:30:49.614057   12428 out.go:179] üöú  Pulling base image v0.0.48 ...
I1103 19:30:49.616466   12428 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1103 19:30:49.616466   12428 image.go:81] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1103 19:30:49.847842   12428 cache.go:152] Downloading docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1103 19:30:49.850514   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\somma\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1103 19:30:49.851197   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\somma\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1103 19:30:49.851790   12428 image.go:65] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1103 19:30:49.852315   12428 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1103 19:30:49.852315   12428 cache.go:58] Caching tarball of preloaded images
I1103 19:30:49.852315   12428 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1103 19:30:49.852315   12428 image.go:68] Found docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I1103 19:30:49.852315   12428 image.go:137] docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I1103 19:30:49.852870   12428 cache.go:155] successfully saved docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1103 19:30:49.852870   12428 cache.go:165] Loading docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1103 19:30:49.852870   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\kic\amd64\stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\somma\.minikube\cache\kic\amd64\stable_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1103 19:30:49.853427   12428 out.go:179] üíæ  Downloading Kubernetes v1.34.0 preload ...
I1103 19:30:49.855407   12428 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1103 19:30:50.959311   12428 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> C:\Users\somma\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1103 19:31:32.292398   12428 cache.go:167] successfully loaded and using docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
W1103 19:32:42.957571   12428 cache.go:64] Error downloading preloaded artifacts will continue without preload: download failed: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35: rename C:\Users\somma\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4.download C:\Users\somma\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4: Acc√®s refus√©.
I1103 19:32:42.957571   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner:v5 -> C:\Users\somma\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5
I1103 19:32:42.957571   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.34.0
I1103 19:32:42.957571   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\etcd:3.6.4-0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.6.4-0
I1103 19:32:42.957571   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns:v1.12.1 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.12.1
I1103 19:32:42.957571   12428 profile.go:143] Saving config to C:\Users\somma\.minikube\profiles\minikube\config.json ...
I1103 19:32:42.957571   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\pause:3.10.1 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\pause_3.10.1
I1103 19:32:42.961217   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.34.0
I1103 19:32:42.961217   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.34.0
I1103 19:32:42.962888   12428 cache.go:232] Successfully downloaded all kic artifacts
I1103 19:32:42.962888   12428 start.go:360] acquireMachinesLock for minikube: {Name:mkcadadfcf7e7a5188787669344e04af97301a0f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:42.964645   12428 start.go:364] duration metric: took 1.7576ms to acquireMachinesLock for "minikube"
I1103 19:32:42.964645   12428 start.go:96] Skipping create...Using existing machine configuration
I1103 19:32:42.964645   12428 fix.go:54] fixHost starting: 
I1103 19:32:42.979991   12428 localpath.go:148] windows sanitize: C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.34.0
I1103 19:32:42.989164   12428 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1103 19:32:43.570367   12428 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1103 19:32:43.570367   12428 fix.go:138] unexpected machine state, will restart: <nil>
I1103 19:32:43.572810   12428 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I1103 19:32:43.581943   12428 cli_runner.go:164] Run: docker start minikube
I1103 19:32:44.528609   12428 cache.go:107] acquiring lock: {Name:mkaacbac7a7b6398b35b7e12b50269299df515df Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.528609   12428 cache.go:107] acquiring lock: {Name:mkf8a5624ab2469bb48cf7fa545f2213bd275410 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.528609   12428 cache.go:107] acquiring lock: {Name:mkceeab36c4f180c7274efa52963bba9d41ab875 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.529666   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\pause_3.10.1 exists
I1103 19:32:44.529666   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.34.0 exists
I1103 19:32:44.529666   12428 cache.go:96] cache image "registry.k8s.io/kube-scheduler:v1.34.0" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-scheduler_v1.34.0" took 1.5684489s
I1103 19:32:44.529666   12428 cache.go:80] save to tar file registry.k8s.io/kube-scheduler:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.34.0 succeeded
I1103 19:32:44.529666   12428 cache.go:96] cache image "registry.k8s.io/pause:3.10.1" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\pause_3.10.1" took 1.5720947s
I1103 19:32:44.529666   12428 cache.go:80] save to tar file registry.k8s.io/pause:3.10.1 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\pause_3.10.1 succeeded
I1103 19:32:44.530186   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.34.0 exists
I1103 19:32:44.531270   12428 cache.go:107] acquiring lock: {Name:mk000d7a657855bb1add5f38f65c20a7b4585f00 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.531270   12428 cache.go:96] cache image "registry.k8s.io/kube-apiserver:v1.34.0" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-apiserver_v1.34.0" took 1.5512791s
I1103 19:32:44.531270   12428 cache.go:80] save to tar file registry.k8s.io/kube-apiserver:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.34.0 succeeded
I1103 19:32:44.531270   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.6.4-0 exists
I1103 19:32:44.532254   12428 cache.go:96] cache image "registry.k8s.io/etcd:3.6.4-0" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\etcd_3.6.4-0" took 1.574683s
I1103 19:32:44.532254   12428 cache.go:80] save to tar file registry.k8s.io/etcd:3.6.4-0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.6.4-0 succeeded
I1103 19:32:44.534310   12428 cache.go:107] acquiring lock: {Name:mkb96e1031fb830b3f2292728f1d8bc13b42ed64 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.534816   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.34.0 exists
I1103 19:32:44.535351   12428 cache.go:96] cache image "registry.k8s.io/kube-controller-manager:v1.34.0" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-controller-manager_v1.34.0" took 1.5741342s
I1103 19:32:44.535351   12428 cache.go:80] save to tar file registry.k8s.io/kube-controller-manager:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.34.0 succeeded
I1103 19:32:44.543836   12428 cache.go:107] acquiring lock: {Name:mka452b6c84cc625f17d6f51678aa16aa1b72e0c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.544343   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.34.0 exists
I1103 19:32:44.544343   12428 cache.go:96] cache image "registry.k8s.io/kube-proxy:v1.34.0" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-proxy_v1.34.0" took 1.5867724s
I1103 19:32:44.544343   12428 cache.go:80] save to tar file registry.k8s.io/kube-proxy:v1.34.0 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.34.0 succeeded
I1103 19:32:44.544883   12428 cache.go:107] acquiring lock: {Name:mkf4c9e5339a88ec15bb1a10e33945692eeebec6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.544883   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.12.1 exists
I1103 19:32:44.544883   12428 cache.go:96] cache image "registry.k8s.io/coredns/coredns:v1.12.1" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\coredns\\coredns_v1.12.1" took 1.5873119s
I1103 19:32:44.544883   12428 cache.go:80] save to tar file registry.k8s.io/coredns/coredns:v1.12.1 -> C:\Users\somma\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.12.1 succeeded
I1103 19:32:44.545913   12428 cache.go:107] acquiring lock: {Name:mka109308b35000591836e523b8fc1c0d1b3a73d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1103 19:32:44.546548   12428 cache.go:115] \\?\Volume{17fa8837-c88e-4fa5-b7be-fd14c883b817}\Users\somma\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5 exists
I1103 19:32:44.546548   12428 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "C:\\Users\\somma\\.minikube\\cache\\images\\amd64\\gcr.io\\k8s-minikube\\storage-provisioner_v5" took 1.5889771s
I1103 19:32:44.546548   12428 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> C:\Users\somma\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5 succeeded
I1103 19:32:44.547087   12428 cache.go:87] Successfully saved all images to host disk.
I1103 19:32:45.143057   12428 cli_runner.go:217] Completed: docker start minikube: (1.5611143s)
I1103 19:32:45.154130   12428 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1103 19:32:45.231655   12428 kic.go:430] container "minikube" state is running.
I1103 19:32:45.245891   12428 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1103 19:32:45.322316   12428 profile.go:143] Saving config to C:\Users\somma\.minikube\profiles\minikube\config.json ...
I1103 19:32:45.325131   12428 machine.go:93] provisionDockerMachine start ...
I1103 19:32:45.336830   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:45.417612   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:45.437337   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:45.437337   12428 main.go:141] libmachine: About to run SSH command:
hostname
I1103 19:32:45.441290   12428 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1103 19:32:48.681237   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1103 19:32:48.681237   12428 ubuntu.go:182] provisioning hostname "minikube"
I1103 19:32:48.693085   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:48.768450   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:48.768450   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:48.768450   12428 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1103 19:32:49.071874   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1103 19:32:49.100223   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:49.260453   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:49.260999   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:49.260999   12428 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1103 19:32:49.444924   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1103 19:32:49.444924   12428 ubuntu.go:188] set auth options {CertDir:C:\Users\somma\.minikube CaCertPath:C:\Users\somma\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\somma\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\somma\.minikube\machines\server.pem ServerKeyPath:C:\Users\somma\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\somma\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\somma\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\somma\.minikube}
I1103 19:32:49.444924   12428 ubuntu.go:190] setting up certificates
I1103 19:32:49.444924   12428 provision.go:84] configureAuth start
I1103 19:32:49.453668   12428 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1103 19:32:49.517447   12428 provision.go:143] copyHostCerts
I1103 19:32:49.534361   12428 exec_runner.go:144] found C:\Users\somma\.minikube/cert.pem, removing ...
I1103 19:32:49.534887   12428 exec_runner.go:203] rm: C:\Users\somma\.minikube\cert.pem
I1103 19:32:49.535618   12428 exec_runner.go:151] cp: C:\Users\somma\.minikube\certs\cert.pem --> C:\Users\somma\.minikube/cert.pem (1119 bytes)
I1103 19:32:49.549324   12428 exec_runner.go:144] found C:\Users\somma\.minikube/key.pem, removing ...
I1103 19:32:49.549324   12428 exec_runner.go:203] rm: C:\Users\somma\.minikube\key.pem
I1103 19:32:49.549324   12428 exec_runner.go:151] cp: C:\Users\somma\.minikube\certs\key.pem --> C:\Users\somma\.minikube/key.pem (1675 bytes)
I1103 19:32:49.565709   12428 exec_runner.go:144] found C:\Users\somma\.minikube/ca.pem, removing ...
I1103 19:32:49.565709   12428 exec_runner.go:203] rm: C:\Users\somma\.minikube\ca.pem
I1103 19:32:49.566413   12428 exec_runner.go:151] cp: C:\Users\somma\.minikube\certs\ca.pem --> C:\Users\somma\.minikube/ca.pem (1074 bytes)
I1103 19:32:49.567980   12428 provision.go:117] generating server cert: C:\Users\somma\.minikube\machines\server.pem ca-key=C:\Users\somma\.minikube\certs\ca.pem private-key=C:\Users\somma\.minikube\certs\ca-key.pem org=somma.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1103 19:32:49.866295   12428 provision.go:177] copyRemoteCerts
I1103 19:32:49.870768   12428 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1103 19:32:49.877641   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:49.939255   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:32:50.078368   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1103 19:32:50.127124   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1103 19:32:50.168338   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1103 19:32:50.209460   12428 provision.go:87] duration metric: took 764.5357ms to configureAuth
I1103 19:32:50.209460   12428 ubuntu.go:206] setting minikube options for container-runtime
I1103 19:32:50.210003   12428 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1103 19:32:50.216114   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:50.278824   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:50.279367   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:50.279367   12428 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1103 19:32:50.485962   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1103 19:32:50.485962   12428 ubuntu.go:71] root file system type: overlay
I1103 19:32:50.486772   12428 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1103 19:32:50.503352   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:50.598683   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:50.599252   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:50.599252   12428 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1103 19:32:50.858978   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1103 19:32:50.871698   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:50.956364   12428 main.go:141] libmachine: Using SSH client type: native
I1103 19:32:50.956937   12428 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7017c0] 0x704300 <nil>  [] 0s} 127.0.0.1 63280 <nil> <nil>}
I1103 19:32:50.956937   12428 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1103 19:32:51.222538   12428 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1103 19:32:51.222538   12428 machine.go:96] duration metric: took 5.8974076s to provisionDockerMachine
I1103 19:32:51.222538   12428 start.go:293] postStartSetup for "minikube" (driver="docker")
I1103 19:32:51.222538   12428 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1103 19:32:51.234428   12428 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1103 19:32:51.271812   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:51.533786   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:32:51.700567   12428 ssh_runner.go:195] Run: cat /etc/os-release
I1103 19:32:51.716573   12428 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1103 19:32:51.716573   12428 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1103 19:32:51.716573   12428 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1103 19:32:51.716573   12428 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1103 19:32:51.716573   12428 filesync.go:126] Scanning C:\Users\somma\.minikube\addons for local assets ...
I1103 19:32:51.717158   12428 filesync.go:126] Scanning C:\Users\somma\.minikube\files for local assets ...
I1103 19:32:51.717789   12428 start.go:296] duration metric: took 495.2507ms for postStartSetup
I1103 19:32:51.741014   12428 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1103 19:32:51.753309   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:51.872038   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:32:52.088654   12428 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1103 19:32:52.122118   12428 fix.go:56] duration metric: took 9.1574727s for fixHost
I1103 19:32:52.122118   12428 start.go:83] releasing machines lock for "minikube", held for 9.1574727s
I1103 19:32:52.153631   12428 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1103 19:32:52.362779   12428 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1103 19:32:52.383910   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:52.387642   12428 ssh_runner.go:195] Run: cat /version.json
I1103 19:32:52.397624   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:32:52.452432   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:32:52.459917   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
W1103 19:32:52.574869   12428 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1103 19:32:52.606758   12428 ssh_runner.go:195] Run: systemctl --version
I1103 19:32:52.664777   12428 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1103 19:32:52.691089   12428 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1103 19:32:52.724287   12428 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1103 19:32:52.734889   12428 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1103 19:32:52.772804   12428 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1103 19:32:52.773391   12428 start.go:495] detecting cgroup driver to use...
I1103 19:32:52.773391   12428 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1103 19:32:52.775176   12428 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1103 19:32:52.868008   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1103 19:32:52.933467   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1103 19:32:52.976135   12428 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1103 19:32:53.000936   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1103 19:32:53.063943   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1103 19:32:53.128776   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1103 19:32:53.194154   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1103 19:32:53.258823   12428 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1103 19:32:53.320196   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1103 19:32:53.384964   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1103 19:32:53.447181   12428 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1103 19:32:53.498659   12428 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1103 19:32:53.537729   12428 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1103 19:32:53.579661   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W1103 19:32:53.639863   12428 out.go:285] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1103 19:32:53.640947   12428 out.go:285] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1103 19:32:53.810408   12428 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1103 19:32:54.011998   12428 start.go:495] detecting cgroup driver to use...
I1103 19:32:54.011998   12428 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1103 19:32:54.020164   12428 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1103 19:32:54.050617   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1103 19:32:54.080028   12428 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1103 19:32:54.122180   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1103 19:32:54.145066   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1103 19:32:54.173007   12428 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1103 19:32:54.242725   12428 ssh_runner.go:195] Run: which cri-dockerd
I1103 19:32:54.254426   12428 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1103 19:32:54.273609   12428 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1103 19:32:54.310336   12428 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1103 19:32:54.443187   12428 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1103 19:32:54.533255   12428 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1103 19:32:54.533255   12428 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1103 19:32:54.566081   12428 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1103 19:32:54.589251   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1103 19:32:54.674593   12428 ssh_runner.go:195] Run: sudo systemctl restart docker
I1103 19:32:57.922484   12428 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.2478912s)
I1103 19:32:57.932564   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1103 19:32:58.014996   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1103 19:32:58.076021   12428 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1103 19:32:58.123129   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1103 19:32:58.148229   12428 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1103 19:32:58.354335   12428 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1103 19:32:58.445116   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1103 19:32:58.565620   12428 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1103 19:32:58.634442   12428 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1103 19:32:58.669736   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1103 19:32:58.764952   12428 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1103 19:32:59.603120   12428 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1103 19:32:59.622820   12428 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1103 19:32:59.633564   12428 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1103 19:32:59.641737   12428 start.go:563] Will wait 60s for crictl version
I1103 19:32:59.651775   12428 ssh_runner.go:195] Run: which crictl
I1103 19:32:59.662667   12428 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1103 19:33:00.006369   12428 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1103 19:33:00.020251   12428 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1103 19:33:00.337844   12428 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1103 19:33:00.382205   12428 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1103 19:33:00.389657   12428 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1103 19:33:00.680144   12428 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1103 19:33:00.702692   12428 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1103 19:33:00.718722   12428 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1103 19:33:00.775689   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1103 19:33:00.872786   12428 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1103 19:33:00.873348   12428 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1103 19:33:00.879759   12428 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1103 19:33:00.919442   12428 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1103 19:33:00.919442   12428 docker.go:621] Images already preloaded, skipping extraction
I1103 19:33:00.926339   12428 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1103 19:33:00.959790   12428 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1103 19:33:00.959790   12428 cache_images.go:85] Images are preloaded, skipping loading
I1103 19:33:00.959790   12428 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1103 19:33:00.960914   12428 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1103 19:33:00.967127   12428 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1103 19:33:01.621917   12428 cni.go:84] Creating CNI manager for ""
I1103 19:33:01.621917   12428 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1103 19:33:01.621917   12428 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1103 19:33:01.621917   12428 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1103 19:33:01.621917   12428 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1103 19:33:01.624784   12428 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1103 19:33:01.645520   12428 binaries.go:44] Found k8s binaries, skipping transfer
I1103 19:33:01.647759   12428 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1103 19:33:01.667061   12428 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1103 19:33:01.696662   12428 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1103 19:33:01.727855   12428 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1103 19:33:01.775008   12428 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1103 19:33:01.783874   12428 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1103 19:33:01.807666   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1103 19:33:01.902233   12428 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1103 19:33:01.970879   12428 certs.go:68] Setting up C:\Users\somma\.minikube\profiles\minikube for IP: 192.168.49.2
I1103 19:33:01.970879   12428 certs.go:194] generating shared ca certs ...
I1103 19:33:01.970879   12428 certs.go:226] acquiring lock for ca certs: {Name:mk4969ab03f1f8891ecee80d861ef957dbb2985f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1103 19:33:01.994249   12428 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\somma\.minikube\ca.key
I1103 19:33:02.031604   12428 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\somma\.minikube\proxy-client-ca.key
I1103 19:33:02.031604   12428 certs.go:256] generating profile certs ...
I1103 19:33:02.032211   12428 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\somma\.minikube\profiles\minikube\client.key
I1103 19:33:02.056626   12428 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\somma\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1103 19:33:02.080108   12428 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\somma\.minikube\profiles\minikube\proxy-client.key
I1103 19:33:02.084064   12428 certs.go:484] found cert: C:\Users\somma\.minikube\certs\ca-key.pem (1675 bytes)
I1103 19:33:02.084869   12428 certs.go:484] found cert: C:\Users\somma\.minikube\certs\ca.pem (1074 bytes)
I1103 19:33:02.084869   12428 certs.go:484] found cert: C:\Users\somma\.minikube\certs\cert.pem (1119 bytes)
I1103 19:33:02.084869   12428 certs.go:484] found cert: C:\Users\somma\.minikube\certs\key.pem (1675 bytes)
I1103 19:33:02.086042   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1103 19:33:02.131536   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1103 19:33:02.174990   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1103 19:33:02.217775   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1103 19:33:02.264632   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1103 19:33:02.307793   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1103 19:33:02.356401   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1103 19:33:02.412222   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1103 19:33:02.725266   12428 ssh_runner.go:362] scp C:\Users\somma\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1103 19:33:03.233750   12428 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1103 19:33:03.581904   12428 ssh_runner.go:195] Run: openssl version
I1103 19:33:03.651965   12428 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1103 19:33:03.721929   12428 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1103 19:33:03.732109   12428 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov  3 18:27 /usr/share/ca-certificates/minikubeCA.pem
I1103 19:33:03.753523   12428 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1103 19:33:03.803587   12428 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1103 19:33:03.946261   12428 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1103 19:33:04.048358   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1103 19:33:04.268588   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1103 19:33:04.664232   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1103 19:33:05.063958   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1103 19:33:05.245229   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1103 19:33:05.370233   12428 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1103 19:33:05.426085   12428 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1103 19:33:05.441260   12428 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1103 19:33:05.607672   12428 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1103 19:33:05.702779   12428 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1103 19:33:05.703589   12428 kubeadm.go:589] restartPrimaryControlPlane start ...
I1103 19:33:05.728578   12428 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1103 19:33:05.849435   12428 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1103 19:33:05.860598   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1103 19:33:05.958722   12428 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\somma\.kube\config
I1103 19:33:05.958722   12428 kubeconfig.go:62] C:\Users\somma\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1103 19:33:05.959318   12428 lock.go:35] WriteFile acquiring C:\Users\somma\.kube\config: {Name:mk0c084687cb485fff8a785b2797ee8221ef714e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1103 19:33:06.038489   12428 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1103 19:33:06.115620   12428 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1103 19:33:06.115620   12428 kubeadm.go:593] duration metric: took 412.0314ms to restartPrimaryControlPlane
I1103 19:33:06.115620   12428 kubeadm.go:394] duration metric: took 689.5346ms to StartCluster
I1103 19:33:06.115620   12428 settings.go:142] acquiring lock: {Name:mkff844175b21bbb6bfa74ddc011df8325fa02d8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1103 19:33:06.116130   12428 settings.go:150] Updating kubeconfig:  C:\Users\somma\.kube\config
I1103 19:33:06.118339   12428 lock.go:35] WriteFile acquiring C:\Users\somma\.kube\config: {Name:mk0c084687cb485fff8a785b2797ee8221ef714e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1103 19:33:06.119393   12428 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1103 19:33:06.119393   12428 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1103 19:33:06.119393   12428 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1103 19:33:06.119393   12428 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1103 19:33:06.119393   12428 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1103 19:33:06.119393   12428 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1103 19:33:06.119393   12428 addons.go:247] addon storage-provisioner should already be in state true
I1103 19:33:06.119393   12428 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1103 19:33:06.120072   12428 host.go:66] Checking if "minikube" exists ...
I1103 19:33:06.120599   12428 out.go:179] üîé  Verifying Kubernetes components...
I1103 19:33:06.126355   12428 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1103 19:33:06.139197   12428 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1103 19:33:06.139713   12428 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1103 19:33:06.202680   12428 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1103 19:33:06.203783   12428 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1103 19:33:06.203783   12428 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1103 19:33:06.204851   12428 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1103 19:33:06.204851   12428 addons.go:247] addon default-storageclass should already be in state true
I1103 19:33:06.204851   12428 host.go:66] Checking if "minikube" exists ...
I1103 19:33:06.212574   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:33:06.224500   12428 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1103 19:33:06.282590   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:33:06.293383   12428 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1103 19:33:06.293383   12428 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1103 19:33:06.305595   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1103 19:33:06.358142   12428 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63280 SSHKeyPath:C:\Users\somma\.minikube\machines\minikube\id_rsa Username:docker}
I1103 19:33:08.021500   12428 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1103 19:33:08.215389   12428 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1103 19:33:08.398051   12428 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (2.2716958s)
I1103 19:33:08.405850   12428 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1103 19:33:11.203194   12428 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.1816946s)
W1103 19:33:11.203194   12428 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1103 19:33:11.203194   12428 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.9878049s)
W1103 19:33:11.203194   12428 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1103 19:33:11.203194   12428 retry.go:31] will retry after 175.470779ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1103 19:33:11.203194   12428 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (2.7973442s)
I1103 19:33:11.203194   12428 retry.go:31] will retry after 370.501103ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1103 19:33:11.244708   12428 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1103 19:33:11.391012   12428 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1103 19:33:11.441471   12428 api_server.go:52] waiting for apiserver process to appear ...
I1103 19:33:11.444496   12428 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1103 19:33:11.583782   12428 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1103 19:33:19.252862   12428 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.8618494s)
I1103 19:33:19.252862   12428 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (7.8083658s)
I1103 19:33:19.252862   12428 api_server.go:72] duration metric: took 13.1334691s to wait for apiserver process to appear ...
I1103 19:33:19.252862   12428 api_server.go:88] waiting for apiserver healthz status ...
I1103 19:33:19.253427   12428 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (7.6696447s)
I1103 19:33:19.253427   12428 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63284/healthz ...
I1103 19:33:19.261609   12428 api_server.go:279] https://127.0.0.1:63284/healthz returned 200:
ok
I1103 19:33:19.273269   12428 api_server.go:141] control plane version: v1.34.0
I1103 19:33:19.273269   12428 api_server.go:131] duration metric: took 20.4071ms to wait for apiserver health ...
I1103 19:33:19.273269   12428 system_pods.go:43] waiting for kube-system pods to appear ...
I1103 19:33:19.278787   12428 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I1103 19:33:19.280452   12428 addons.go:514] duration metric: took 13.161059s for enable addons: enabled=[storage-provisioner default-storageclass]
I1103 19:33:19.286439   12428 system_pods.go:59] 7 kube-system pods found
I1103 19:33:19.286439   12428 system_pods.go:61] "coredns-66bc5c9577-sdrcw" [7dc713a7-beb1-4bd3-87e5-2ea3f3643f71] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "etcd-minikube" [2aa0d34a-7b62-487e-9d18-018312aab285] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "kube-apiserver-minikube" [39d8fa97-3f04-4461-8526-d9f02ee51f0c] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "kube-controller-manager-minikube" [4927571b-1a0c-430f-a800-9eb744d1c94d] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "kube-proxy-gm5s6" [67906f7e-14ad-48b5-ba00-183fea7bf701] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "kube-scheduler-minikube" [e50f9a06-50c4-47b5-a61e-0a6b570b5893] Running
I1103 19:33:19.286439   12428 system_pods.go:61] "storage-provisioner" [b29353e0-b85d-422f-a8ab-fce4199dd4fa] Running
I1103 19:33:19.286439   12428 system_pods.go:74] duration metric: took 13.1698ms to wait for pod list to return data ...
I1103 19:33:19.286439   12428 kubeadm.go:578] duration metric: took 13.167046s to wait for: map[apiserver:true system_pods:true]
I1103 19:33:19.286439   12428 node_conditions.go:102] verifying NodePressure condition ...
I1103 19:33:19.291817   12428 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1103 19:33:19.291817   12428 node_conditions.go:123] node cpu capacity is 8
I1103 19:33:19.291817   12428 node_conditions.go:105] duration metric: took 5.3784ms to run NodePressure ...
I1103 19:33:19.291817   12428 start.go:241] waiting for startup goroutines ...
I1103 19:33:19.291817   12428 start.go:246] waiting for cluster config update ...
I1103 19:33:19.291817   12428 start.go:255] writing updated cluster config ...
I1103 19:33:19.308554   12428 ssh_runner.go:195] Run: rm -f paused
I1103 19:33:19.422931   12428 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1103 19:33:19.424068   12428 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 03 19:32:54 minikube dockerd[804]: time="2025-11-03T19:32:54.964933219Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 03 19:32:54 minikube dockerd[804]: time="2025-11-03T19:32:54.964953922Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 03 19:32:54 minikube dockerd[804]: time="2025-11-03T19:32:54.987598130Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 03 19:32:54 minikube dockerd[804]: time="2025-11-03T19:32:54.999159962Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 03 19:32:55 minikube dockerd[804]: time="2025-11-03T19:32:55.035404537Z" level=info msg="Loading containers: start."
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.417295321Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 9816cf93a048991c15f0ce053d73fa49994312bebb670a3c9832ff98503e2d85], retrying...."
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679402970Z" level=warning msg="error locating sandbox id ce4577a59a2d45d0d215eba32e4a95b94b4dc058b6a5435dc712fee412f7466f: sandbox ce4577a59a2d45d0d215eba32e4a95b94b4dc058b6a5435dc712fee412f7466f not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679622020Z" level=warning msg="error locating sandbox id 3bc026d72a787032588ea64b8a0b7342a003337feea4c67b73494305a208d585: sandbox 3bc026d72a787032588ea64b8a0b7342a003337feea4c67b73494305a208d585 not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679707740Z" level=warning msg="error locating sandbox id 67d1582836f685585506dfdeb9677f4f858a2d63eff96a2c79b67206b7cd95f2: sandbox 67d1582836f685585506dfdeb9677f4f858a2d63eff96a2c79b67206b7cd95f2 not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679759152Z" level=warning msg="error locating sandbox id d011dfe344e931f64d0fa98a6f908ef81d4bdbbd571fea71e36b54bf4e84b9b1: sandbox d011dfe344e931f64d0fa98a6f908ef81d4bdbbd571fea71e36b54bf4e84b9b1 not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679796960Z" level=warning msg="error locating sandbox id 7460a20d2c58045a9eea808786aac3dc255530703e72548c47b2daafc2f97e8a: sandbox 7460a20d2c58045a9eea808786aac3dc255530703e72548c47b2daafc2f97e8a not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679843971Z" level=warning msg="error locating sandbox id e53978114dbca3f4cd7f2bb68866bdd5c8531e3348244d33d5251e7eb51c1c59: sandbox e53978114dbca3f4cd7f2bb68866bdd5c8531e3348244d33d5251e7eb51c1c59 not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.679942494Z" level=warning msg="error locating sandbox id b59949aa3cd3beafa2bef9f6bd69cde379ca4a737348d473bdde87dddf859083: sandbox b59949aa3cd3beafa2bef9f6bd69cde379ca4a737348d473bdde87dddf859083 not found"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.681384023Z" level=info msg="Loading containers: done."
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.752357757Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.752757248Z" level=info msg="Initializing buildkit"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.886628067Z" level=info msg="Completed buildkit initialization"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.906161635Z" level=info msg="Daemon has completed initialization"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.906558525Z" level=info msg="API listen on /run/docker.sock"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.906605636Z" level=info msg="API listen on /var/run/docker.sock"
Nov 03 19:32:57 minikube dockerd[804]: time="2025-11-03T19:32:57.906635843Z" level=info msg="API listen on [::]:2376"
Nov 03 19:32:57 minikube systemd[1]: Started Docker Application Container Engine.
Nov 03 19:32:58 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Start docker client with request timeout 0s"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Loaded network plugin cni"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 03 19:32:59 minikube cri-dockerd[1123]: time="2025-11-03T19:32:59Z" level=info msg="Start cri-dockerd grpc backend"
Nov 03 19:32:59 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 03 19:33:03 minikube cri-dockerd[1123]: time="2025-11-03T19:33:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-sdrcw_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b2740314e2a0e03ebb5e5b0469f3fe4c9fced346781e1dce80f97a7bb9dba97b\""
Nov 03 19:33:05 minikube cri-dockerd[1123]: time="2025-11-03T19:33:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be65306baf80bdbf3f1843e69f1cc66436da38dd76e3fd474567bb74b78f8a20/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:05 minikube cri-dockerd[1123]: time="2025-11-03T19:33:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/189ceca0ced6dbb3a772098b2d80e45702c918e6f21dbc0525d4f98a6c55e184/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:05 minikube cri-dockerd[1123]: time="2025-11-03T19:33:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1fb65c5b1763e78a01e568f9382ef0760aae661bbb09c87b41610d1ef9c589b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:05 minikube cri-dockerd[1123]: time="2025-11-03T19:33:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22e77010e1fef31fff09342e40e718541bca33f115252db577bd4584ece1cf17/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:17 minikube cri-dockerd[1123]: time="2025-11-03T19:33:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 03 19:33:22 minikube cri-dockerd[1123]: time="2025-11-03T19:33:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/32b37906dff18a13c48dd1f2c8b4a83524dcbdec21ce778527a5801e33e22bd9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:22 minikube cri-dockerd[1123]: time="2025-11-03T19:33:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aed3f259090cbf574788a198f50eee843c40392bec81f0a4894b1cd2a29ccf50/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:22 minikube cri-dockerd[1123]: time="2025-11-03T19:33:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6fcdcd5ab8f875dc89bd96e2a47e6ef746b315f736cb236dc3580c86bf8e06db/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 03 19:33:44 minikube dockerd[804]: time="2025-11-03T19:33:44.449830819Z" level=info msg="ignoring event" container=3574593408c1ebbe625f60a28e3de2d2e926e6b72637d2442ab77c11c3606b86 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 19:34:18 minikube cri-dockerd[1123]: time="2025-11-03T19:34:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98b20c54e217f99fe1f5236e4723a88851fd78976a12ff4fc81187db61fb00c4/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 03 19:34:18 minikube cri-dockerd[1123]: time="2025-11-03T19:34:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9c0cddb00244d1b4171d2dc7d937b450c5af023acfecd7c24d9dc0d46a5161b1/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 03 19:34:18 minikube dockerd[804]: time="2025-11-03T19:34:18.982010070Z" level=warning msg="reference for unknown type: " digest="sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Nov 03 19:34:30 minikube cri-dockerd[1123]: time="2025-11-03T19:34:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=>                                                 ]    834kB/27.61MB"
Nov 03 19:34:40 minikube cri-dockerd[1123]: time="2025-11-03T19:34:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [==============>                                    ]  7.797MB/27.61MB"
Nov 03 19:34:48 minikube cri-dockerd[1123]: time="2025-11-03T19:34:48Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Nov 03 19:34:48 minikube dockerd[804]: time="2025-11-03T19:34:48.912075379Z" level=info msg="ignoring event" container=1db49ae0327d663626f6944cc65394d7ec1722cb5ddca8e91771e695b259d5b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 19:34:49 minikube cri-dockerd[1123]: time="2025-11-03T19:34:49Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Nov 03 19:34:49 minikube dockerd[804]: time="2025-11-03T19:34:49.336279641Z" level=info msg="ignoring event" container=b81a8f5aaefb21077301b65747e5fad5c3cc8108c8826c350ba7f95d6068c90d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 19:34:50 minikube dockerd[804]: time="2025-11-03T19:34:50.354796793Z" level=info msg="ignoring event" container=98b20c54e217f99fe1f5236e4723a88851fd78976a12ff4fc81187db61fb00c4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 19:34:51 minikube dockerd[804]: time="2025-11-03T19:34:51.458496526Z" level=info msg="ignoring event" container=9c0cddb00244d1b4171d2dc7d937b450c5af023acfecd7c24d9dc0d46a5161b1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 03 19:35:21 minikube cri-dockerd[1123]: time="2025-11-03T19:35:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e9cf69e050d909ac86baba5c951860d2f324f1149c69f4e99399ee6bc05ed86e/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 03 19:35:21 minikube dockerd[804]: time="2025-11-03T19:35:21.924898030Z" level=warning msg="reference for unknown type: " digest="sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef" remote="registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef"
Nov 03 19:35:33 minikube cri-dockerd[1123]: time="2025-11-03T19:35:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [=============================================>     ]  30.32MB/33.68MB"
Nov 03 19:35:43 minikube cri-dockerd[1123]: time="2025-11-03T19:35:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [========================================>          ]  17.87MB/22.11MB"
Nov 03 19:35:45 minikube cri-dockerd[1123]: time="2025-11-03T19:35:45Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
25ef11fb647c7       registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef             About a minute ago   Running             controller                0                   e9cf69e050d90       ingress-nginx-controller-9cc49f96f-5g62x
b81a8f5aaefb2       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24   2 minutes ago        Exited              patch                     0                   9c0cddb00244d       ingress-nginx-admission-patch-krwv8
1db49ae0327d6       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24   2 minutes ago        Exited              create                    0                   98b20c54e217f       ingress-nginx-admission-create-gcw4r
1ad45c6324a22       6e38f40d628db                                                                                                                2 minutes ago        Running             storage-provisioner       2                   32b37906dff18       storage-provisioner
07cb375c90ce2       52546a367cc9e                                                                                                                3 minutes ago        Running             coredns                   1                   6fcdcd5ab8f87       coredns-66bc5c9577-sdrcw
3574593408c1e       6e38f40d628db                                                                                                                3 minutes ago        Exited              storage-provisioner       1                   32b37906dff18       storage-provisioner
a04348dfc3e54       df0860106674d                                                                                                                3 minutes ago        Running             kube-proxy                1                   aed3f259090cb       kube-proxy-gm5s6
e7b37d4d7028d       90550c43ad2bc                                                                                                                3 minutes ago        Running             kube-apiserver            1                   189ceca0ced6d       kube-apiserver-minikube
af236b00bfec6       46169d968e920                                                                                                                3 minutes ago        Running             kube-scheduler            1                   22e77010e1fef       kube-scheduler-minikube
cad9e7aeec954       a0af72f2ec6d6                                                                                                                3 minutes ago        Running             kube-controller-manager   1                   be65306baf80b       kube-controller-manager-minikube
a53582e6ee5ce       5f1f5298c888d                                                                                                                3 minutes ago        Running             etcd                      1                   a1fb65c5b1763       etcd-minikube
e7b19015ca95a       52546a367cc9e                                                                                                                About an hour ago    Exited              coredns                   0                   b2740314e2a0e       coredns-66bc5c9577-sdrcw
cedeee1374321       df0860106674d                                                                                                                About an hour ago    Exited              kube-proxy                0                   2e2f11e2c4969       kube-proxy-gm5s6
df76a31cdf053       90550c43ad2bc                                                                                                                About an hour ago    Exited              kube-apiserver            0                   3f5b9c44488f1       kube-apiserver-minikube
29fddeaece407       46169d968e920                                                                                                                About an hour ago    Exited              kube-scheduler            0                   b7bd81927b6fa       kube-scheduler-minikube
58e9e434f2562       a0af72f2ec6d6                                                                                                                About an hour ago    Exited              kube-controller-manager   0                   19e3b7c0d71f2       kube-controller-manager-minikube
ceb0b50e5eb95       5f1f5298c888d                                                                                                                About an hour ago    Exited              etcd                      0                   25a5809dd9114       etcd-minikube


==> controller_ingress [25ef11fb647c] <==
W1103 19:35:45.886804       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I1103 19:35:45.889491       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I1103 19:35:45.895714       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="34" git="v1.34.0" state="clean" commit="f28b4c9efbca5c5c0af716d9f2d5702667ee8a45" platform="linux/amd64"
I1103 19:35:46.086360       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I1103 19:35:46.100586       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I1103 19:35:46.112133       7 nginx.go:273] "Starting NGINX Ingress controller"
I1103 19:35:46.117213       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"49d8791c-eb68-4061-ab8d-20bba39a4420", APIVersion:"v1", ResourceVersion:"898", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I1103 19:35:46.118755       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"1a9aebf7-2f4e-44e1-9ed6-08b8a9d422f1", APIVersion:"v1", ResourceVersion:"899", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I1103 19:35:46.118835       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"c6b31018-318a-405a-a3af-c9527a1b8323", APIVersion:"v1", ResourceVersion:"900", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I1103 19:35:47.308707       7 nginx.go:319] "Starting NGINX process"
I1103 19:35:47.308858       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I1103 19:35:47.309835       7 nginx.go:339] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I1103 19:35:47.310685       7 controller.go:214] "Configuration changes detected, backend reload required"
I1103 19:35:47.322081       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I1103 19:35:47.322142       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-9cc49f96f-5g62x"
I1103 19:35:47.329732       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-5g62x" node="minikube"
I1103 19:35:47.363738       7 controller.go:228] "Backend successfully reloaded"
I1103 19:35:47.363885       7 controller.go:240] "Initial sync, sleeping for 1 second"
I1103 19:35:47.364016       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-9cc49f96f-5g62x", UID:"4caa3a3a-7f12-4f62-b7da-8136bd21d59d", APIVersion:"v1", ResourceVersion:"1045", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I1103 19:35:47.470714       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-5g62x" node="minikube"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.13.2
  Build:         11c69a64ce3c5bdfb6782434d9f62296d4b42179
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.27.1

-------------------------------------------------------------------------------



==> coredns [07cb375c90ce] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:36155 - 26543 "HINFO IN 3671837041740177535.653659879351200039. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.136987188s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [e7b19015ca95] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
[INFO] Reloading complete
[INFO] 127.0.0.1:45618 - 8021 "HINFO IN 5026534172935869537.1486933052990717003. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.084321212s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_03T18_27_28_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 03 Nov 2025 18:27:20 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 03 Nov 2025 19:36:51 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 03 Nov 2025 19:36:11 +0000   Mon, 03 Nov 2025 18:27:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 03 Nov 2025 19:36:11 +0000   Mon, 03 Nov 2025 18:27:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 03 Nov 2025 19:36:11 +0000   Mon, 03 Nov 2025 18:27:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 03 Nov 2025 19:36:11 +0000   Mon, 03 Nov 2025 18:27:21 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8059096Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8059096Ki
  pods:               110
System Info:
  Machine ID:                 f45b124dabd341dba93b391e137566be
  System UUID:                f45b124dabd341dba93b391e137566be
  Boot ID:                    1b28e10e-85f0-4e28-aa88-aa9a21b85d41
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-9cc49f96f-5g62x    100m (1%)     0 (0%)      90Mi (1%)        0 (0%)         2m41s
  kube-system                 coredns-66bc5c9577-sdrcw                    100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     69m
  kube-system                 etcd-minikube                               100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         69m
  kube-system                 kube-apiserver-minikube                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         69m
  kube-system                 kube-controller-manager-minikube            200m (2%)     0 (0%)      0 (0%)           0 (0%)         69m
  kube-system                 kube-proxy-gm5s6                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m
  kube-system                 kube-scheduler-minikube                     100m (1%)     0 (0%)      0 (0%)           0 (0%)         69m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         69m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             260Mi (3%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 69m                    kube-proxy       
  Normal   Starting                 3m33s                  kube-proxy       
  Normal   NodeHasSufficientMemory  69m (x8 over 69m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    69m (x8 over 69m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     69m (x7 over 69m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  69m                    kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 69m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  69m                    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    69m                    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     69m                    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  69m                    kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           69m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 3m55s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  3m55s (x8 over 3m55s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    3m55s (x8 over 3m55s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     3m55s (x7 over 3m55s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  3m55s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 3m40s                  kubelet          Node minikube has been rebooted, boot id: 1b28e10e-85f0-4e28-aa88-aa9a21b85d41
  Normal   RegisteredNode           3m36s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov 3 18:36] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.028560] PCI: Fatal: No config space access function found
[  +0.042792] PCI: System does not support PCI
[  +0.209933] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +3.978181] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.021628] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Africa/Ouagadougou not found. Is the tzdata package installed?
[  +0.439182] pulseaudio[265]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.403599] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.021927] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002797] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003556] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001817] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.775045] WSL (216) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.360043] netlink: 'init': attribute type 4 has an invalid length.
[  +0.203466] virtiofs: Unknown parameter 'negative_dentry_timeout'
[Nov 3 19:23] WSL (216) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [a53582e6ee5c] <==
{"level":"warn","ts":"2025-11-03T19:33:14.640182Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33464","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.696520Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33468","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.704289Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33498","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.714113Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33512","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.723105Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33534","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.820184Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33576","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.829182Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33590","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.889883Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33596","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.900609Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33636","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.910081Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.919284Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33666","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.928237Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33680","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:14.990429Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33698","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.000613Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33726","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.012077Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.020695Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33750","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.029401Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33770","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.198931Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33794","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.208448Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33806","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.218157Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33820","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.227764Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33842","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.298388Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33856","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.309886Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33866","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.319347Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33884","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.328489Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33896","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.401782Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33924","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.419542Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33938","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.429811Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33940","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.440431Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33954","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.522551Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:33996","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.608436Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34006","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.690918Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34016","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.721667Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34040","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.798137Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34062","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.806546Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34076","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.815276Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34106","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.826446Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.834009Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34138","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.894557Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34166","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.902493Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34186","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.935630Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34208","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.944254Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34234","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:15.994286Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34244","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-03T19:33:16.122273Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:34288","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-03T19:33:21.612172Z","caller":"traceutil/trace.go:172","msg":"trace[352615859] transaction","detail":"{read_only:false; response_revision:829; number_of_response:1; }","duration":"107.059105ms","start":"2025-11-03T19:33:21.502015Z","end":"2025-11-03T19:33:21.609074Z","steps":["trace[352615859] 'process raft request'  (duration: 90.196086ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T19:33:21.796380Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"104.368208ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128041075308036086 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/kube-system/kube-dns-nvkr5\" mod_revision:471 > success:<request_put:<key:\"/registry/endpointslices/kube-system/kube-dns-nvkr5\" value_size:1016 >> failure:<request_range:<key:\"/registry/endpointslices/kube-system/kube-dns-nvkr5\" > >>","response":"size:16"}
{"level":"info","ts":"2025-11-03T19:33:21.797199Z","caller":"traceutil/trace.go:172","msg":"trace[518765563] transaction","detail":"{read_only:false; response_revision:830; number_of_response:1; }","duration":"293.471691ms","start":"2025-11-03T19:33:21.503588Z","end":"2025-11-03T19:33:21.797060Z","steps":["trace[518765563] 'process raft request'  (duration: 184.388411ms)","trace[518765563] 'compare'  (duration: 10.8108ms)","trace[518765563] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/endpointslices/kube-system/kube-dns-nvkr5; req_size:1072; } (duration: 93.231047ms)"],"step_count":3}
{"level":"info","ts":"2025-11-03T19:33:21.798296Z","caller":"traceutil/trace.go:172","msg":"trace[1402419117] transaction","detail":"{read_only:false; response_revision:831; number_of_response:1; }","duration":"180.684526ms","start":"2025-11-03T19:33:21.617061Z","end":"2025-11-03T19:33:21.797746Z","steps":["trace[1402419117] 'process raft request'  (duration: 179.824166ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T19:33:21.803226Z","caller":"traceutil/trace.go:172","msg":"trace[1774625000] transaction","detail":"{read_only:false; response_revision:832; number_of_response:1; }","duration":"105.249971ms","start":"2025-11-03T19:33:21.697908Z","end":"2025-11-03T19:33:21.803157Z","steps":["trace[1774625000] 'process raft request'  (duration: 104.97732ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T19:34:17.094108Z","caller":"traceutil/trace.go:172","msg":"trace[212423299] transaction","detail":"{read_only:false; response_revision:934; number_of_response:1; }","duration":"102.620423ms","start":"2025-11-03T19:34:16.991432Z","end":"2025-11-03T19:34:17.094053Z","steps":["trace[212423299] 'process raft request'  (duration: 102.455598ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T19:34:17.094894Z","caller":"traceutil/trace.go:172","msg":"trace[564326854] transaction","detail":"{read_only:false; response_revision:933; number_of_response:1; }","duration":"104.259173ms","start":"2025-11-03T19:34:16.990293Z","end":"2025-11-03T19:34:17.094552Z","steps":["trace[564326854] 'process raft request'  (duration: 88.520677ms)","trace[564326854] 'compare'  (duration: 14.436698ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T19:34:17.099436Z","caller":"traceutil/trace.go:172","msg":"trace[1743607417] transaction","detail":"{read_only:false; response_revision:935; number_of_response:1; }","duration":"101.592768ms","start":"2025-11-03T19:34:16.997788Z","end":"2025-11-03T19:34:17.099381Z","steps":["trace[1743607417] 'process raft request'  (duration: 101.243514ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T19:35:45.658266Z","caller":"traceutil/trace.go:172","msg":"trace[1516447434] linearizableReadLoop","detail":"{readStateIndex:1180; appliedIndex:1180; }","duration":"329.160325ms","start":"2025-11-03T19:35:45.329081Z","end":"2025-11-03T19:35:45.658242Z","steps":["trace[1516447434] 'read index received'  (duration: 329.153824ms)","trace[1516447434] 'applied index is now lower than readState.Index'  (duration: 5.4¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-11-03T19:35:45.658433Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"329.32454ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-03T19:35:45.658482Z","caller":"traceutil/trace.go:172","msg":"trace[1967983203] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1041; }","duration":"329.395847ms","start":"2025-11-03T19:35:45.329076Z","end":"2025-11-03T19:35:45.658472Z","steps":["trace[1967983203] 'agreement among raft nodes before linearized reading'  (duration: 329.287436ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T19:35:45.658530Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-11-03T19:35:45.329056Z","time spent":"329.452352ms","remote":"127.0.0.1:33482","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/pods\" limit:1 "}
{"level":"warn","ts":"2025-11-03T19:35:45.658649Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"186.098723ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-03T19:35:45.658756Z","caller":"traceutil/trace.go:172","msg":"trace[1092639805] transaction","detail":"{read_only:false; response_revision:1042; number_of_response:1; }","duration":"436.678421ms","start":"2025-11-03T19:35:45.222041Z","end":"2025-11-03T19:35:45.658720Z","steps":["trace[1092639805] 'process raft request'  (duration: 436.310386ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T19:35:45.658748Z","caller":"traceutil/trace.go:172","msg":"trace[759089798] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1042; }","duration":"186.202333ms","start":"2025-11-03T19:35:45.472533Z","end":"2025-11-03T19:35:45.658736Z","steps":["trace[759089798] 'agreement among raft nodes before linearized reading'  (duration: 186.079521ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T19:35:45.658876Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-11-03T19:35:45.222019Z","time spent":"436.781031ms","remote":"127.0.0.1:33458","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1040 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}


==> etcd [ceb0b50e5eb9] <==
{"level":"info","ts":"2025-11-03T18:27:21.244170Z","caller":"traceutil/trace.go:172","msg":"trace[1640192409] transaction","detail":"{read_only:false; response_revision:14; number_of_response:1; }","duration":"293.240736ms","start":"2025-11-03T18:27:20.950857Z","end":"2025-11-03T18:27:21.244098Z","steps":["trace[1640192409] 'process raft request'  (duration: 206.670171ms)","trace[1640192409] 'compare'  (duration: 85.734984ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:21.244557Z","caller":"traceutil/trace.go:172","msg":"trace[1426611262] transaction","detail":"{read_only:false; response_revision:15; number_of_response:1; }","duration":"292.36325ms","start":"2025-11-03T18:27:20.952132Z","end":"2025-11-03T18:27:21.244496Z","steps":["trace[1426611262] 'process raft request'  (duration: 291.705487ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:21.246106Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"182.023189ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/minikube\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"warn","ts":"2025-11-03T18:27:21.246653Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"188.196285ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-11-03T18:27:21.246825Z","caller":"traceutil/trace.go:172","msg":"trace[1749270975] range","detail":"{range_begin:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; range_end:; response_count:0; response_revision:17; }","duration":"188.520516ms","start":"2025-11-03T18:27:21.058262Z","end":"2025-11-03T18:27:21.246782Z","steps":["trace[1749270975] 'agreement among raft nodes before linearized reading'  (duration: 188.14128ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:21.246036Z","caller":"traceutil/trace.go:172","msg":"trace[1084043603] transaction","detail":"{read_only:false; response_revision:16; number_of_response:1; }","duration":"183.181601ms","start":"2025-11-03T18:27:21.062808Z","end":"2025-11-03T18:27:21.245989Z","steps":["trace[1084043603] 'process raft request'  (duration: 181.336922ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:21.246678Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"295.484053ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-11-03T18:27:21.248104Z","caller":"traceutil/trace.go:172","msg":"trace[255419312] range","detail":"{range_begin:/registry/certificatesigningrequests; range_end:; response_count:0; response_revision:17; }","duration":"296.898689ms","start":"2025-11-03T18:27:20.951166Z","end":"2025-11-03T18:27:21.248065Z","steps":["trace[255419312] 'agreement among raft nodes before linearized reading'  (duration: 295.430247ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:21.246251Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"295.996102ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:1 size:350"}
{"level":"info","ts":"2025-11-03T18:27:21.249450Z","caller":"traceutil/trace.go:172","msg":"trace[1183509722] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:17; }","duration":"299.088001ms","start":"2025-11-03T18:27:20.950213Z","end":"2025-11-03T18:27:21.249301Z","steps":["trace[1183509722] 'agreement among raft nodes before linearized reading'  (duration: 295.748678ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:21.246514Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"181.639851ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-11-03T18:27:21.251057Z","caller":"traceutil/trace.go:172","msg":"trace[1764690404] range","detail":"{range_begin:/registry/limitranges; range_end:; response_count:0; response_revision:17; }","duration":"186.002173ms","start":"2025-11-03T18:27:21.064842Z","end":"2025-11-03T18:27:21.250844Z","steps":["trace[1764690404] 'agreement among raft nodes before linearized reading'  (duration: 181.593847ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:21.246665Z","caller":"traceutil/trace.go:172","msg":"trace[374699276] range","detail":"{range_begin:/registry/csinodes/minikube; range_end:; response_count:0; response_revision:17; }","duration":"182.580343ms","start":"2025-11-03T18:27:21.064031Z","end":"2025-11-03T18:27:21.246612Z","steps":["trace[374699276] 'agreement among raft nodes before linearized reading'  (duration: 181.92948ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:21.246112Z","caller":"traceutil/trace.go:172","msg":"trace[908478733] transaction","detail":"{read_only:false; response_revision:17; number_of_response:1; }","duration":"101.875144ms","start":"2025-11-03T18:27:21.144079Z","end":"2025-11-03T18:27:21.245954Z","steps":["trace[908478733] 'process raft request'  (duration: 101.709428ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:22.455565Z","caller":"traceutil/trace.go:172","msg":"trace[1100859669] transaction","detail":"{read_only:false; response_revision:83; number_of_response:1; }","duration":"107.52559ms","start":"2025-11-03T18:27:22.347972Z","end":"2025-11-03T18:27:22.455497Z","steps":["trace[1100859669] 'process raft request'  (duration: 14.256378ms)","trace[1100859669] 'compare'  (duration: 92.790466ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:22.660013Z","caller":"traceutil/trace.go:172","msg":"trace[1186686023] transaction","detail":"{read_only:false; response_revision:85; number_of_response:1; }","duration":"103.625013ms","start":"2025-11-03T18:27:22.556331Z","end":"2025-11-03T18:27:22.659956Z","steps":["trace[1186686023] 'process raft request'  (duration: 103.218574ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:22.741360Z","caller":"traceutil/trace.go:172","msg":"trace[25765876] transaction","detail":"{read_only:false; response_revision:86; number_of_response:1; }","duration":"184.959273ms","start":"2025-11-03T18:27:22.556345Z","end":"2025-11-03T18:27:22.741305Z","steps":["trace[25765876] 'process raft request'  (duration: 183.58394ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:28.656827Z","caller":"traceutil/trace.go:172","msg":"trace[1972782996] transaction","detail":"{read_only:false; number_of_response:0; response_revision:305; }","duration":"114.827703ms","start":"2025-11-03T18:27:28.541932Z","end":"2025-11-03T18:27:28.656759Z","steps":["trace[1972782996] 'process raft request'  (duration: 100.160498ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:28.856516Z","caller":"traceutil/trace.go:172","msg":"trace[535996995] transaction","detail":"{read_only:false; response_revision:308; number_of_response:1; }","duration":"102.89996ms","start":"2025-11-03T18:27:28.753563Z","end":"2025-11-03T18:27:28.856463Z","steps":["trace[535996995] 'compare'  (duration: 92.116327ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:28.941095Z","caller":"traceutil/trace.go:172","msg":"trace[1970669866] linearizableReadLoop","detail":"{readStateIndex:315; appliedIndex:314; }","duration":"178.96735ms","start":"2025-11-03T18:27:28.761950Z","end":"2025-11-03T18:27:28.940917Z","steps":["trace[1970669866] 'read index received'  (duration: 112.011¬µs)","trace[1970669866] 'applied index is now lower than readState.Index'  (duration: 178.849238ms)"],"step_count":2}
{"level":"warn","ts":"2025-11-03T18:27:28.942097Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"179.287381ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:2776"}
{"level":"info","ts":"2025-11-03T18:27:28.942240Z","caller":"traceutil/trace.go:172","msg":"trace[6700876] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:309; }","duration":"179.461497ms","start":"2025-11-03T18:27:28.762742Z","end":"2025-11-03T18:27:28.942203Z","steps":["trace[6700876] 'agreement among raft nodes before linearized reading'  (duration: 178.851939ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:28.946845Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"189.727081ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/etcd-minikube\" limit:1 ","response":"range_response_count:1 size:4364"}
{"level":"info","ts":"2025-11-03T18:27:28.947115Z","caller":"traceutil/trace.go:172","msg":"trace[1092850764] range","detail":"{range_begin:/registry/pods/kube-system/etcd-minikube; range_end:; response_count:1; response_revision:309; }","duration":"190.027009ms","start":"2025-11-03T18:27:28.757040Z","end":"2025-11-03T18:27:28.947067Z","steps":["trace[1092850764] 'agreement among raft nodes before linearized reading'  (duration: 184.360566ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:29.259229Z","caller":"traceutil/trace.go:172","msg":"trace[1107796078] transaction","detail":"{read_only:false; number_of_response:0; response_revision:309; }","duration":"105.802439ms","start":"2025-11-03T18:27:29.153272Z","end":"2025-11-03T18:27:29.259075Z","steps":["trace[1107796078] 'process raft request'  (duration: 105.656725ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:29.562161Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"102.68074ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/ttl-controller\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-03T18:27:29.563337Z","caller":"traceutil/trace.go:172","msg":"trace[2025753073] transaction","detail":"{read_only:false; response_revision:312; number_of_response:1; }","duration":"116.282543ms","start":"2025-11-03T18:27:29.446856Z","end":"2025-11-03T18:27:29.563139Z","steps":["trace[2025753073] 'process raft request'  (duration: 95.681269ms)","trace[2025753073] 'compare'  (duration: 19.51477ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:29.563355Z","caller":"traceutil/trace.go:172","msg":"trace[260193530] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/ttl-controller; range_end:; response_count:0; response_revision:311; }","duration":"103.980864ms","start":"2025-11-03T18:27:29.458886Z","end":"2025-11-03T18:27:29.562867Z","steps":["trace[260193530] 'agreement among raft nodes before linearized reading'  (duration: 84.824928ms)","trace[260193530] 'range keys from in-memory index tree'  (duration: 17.777203ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:29.562630Z","caller":"traceutil/trace.go:172","msg":"trace[140900213] transaction","detail":"{read_only:false; response_revision:313; number_of_response:1; }","duration":"101.488626ms","start":"2025-11-03T18:27:29.461092Z","end":"2025-11-03T18:27:29.562581Z","steps":["trace[140900213] 'process raft request'  (duration: 101.240302ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.061440Z","caller":"traceutil/trace.go:172","msg":"trace[41681443] transaction","detail":"{read_only:false; response_revision:345; number_of_response:1; }","duration":"102.911661ms","start":"2025-11-03T18:27:31.958497Z","end":"2025-11-03T18:27:32.061408Z","steps":["trace[41681443] 'process raft request'  (duration: 101.261703ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.062350Z","caller":"traceutil/trace.go:172","msg":"trace[1747948481] transaction","detail":"{read_only:false; response_revision:346; number_of_response:1; }","duration":"103.764143ms","start":"2025-11-03T18:27:31.958558Z","end":"2025-11-03T18:27:32.062322Z","steps":["trace[1747948481] 'process raft request'  (duration: 101.348911ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.152235Z","caller":"traceutil/trace.go:172","msg":"trace[2070594978] transaction","detail":"{read_only:false; response_revision:348; number_of_response:1; }","duration":"109.985739ms","start":"2025-11-03T18:27:32.042199Z","end":"2025-11-03T18:27:32.152184Z","steps":["trace[2070594978] 'process raft request'  (duration: 109.507893ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.153560Z","caller":"traceutil/trace.go:172","msg":"trace[1277530557] transaction","detail":"{read_only:false; response_revision:349; number_of_response:1; }","duration":"106.902944ms","start":"2025-11-03T18:27:32.046604Z","end":"2025-11-03T18:27:32.153507Z","steps":["trace[1277530557] 'process raft request'  (duration: 105.229383ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.156698Z","caller":"traceutil/trace.go:172","msg":"trace[284478237] transaction","detail":"{read_only:false; response_revision:347; number_of_response:1; }","duration":"193.806272ms","start":"2025-11-03T18:27:31.962831Z","end":"2025-11-03T18:27:32.156637Z","steps":["trace[284478237] 'process raft request'  (duration: 188.517365ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:32.467747Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"114.332156ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-11-03T18:27:32.539377Z","caller":"traceutil/trace.go:172","msg":"trace[1442412674] transaction","detail":"{read_only:false; response_revision:356; number_of_response:1; }","duration":"180.22267ms","start":"2025-11-03T18:27:32.359083Z","end":"2025-11-03T18:27:32.539306Z","steps":["trace[1442412674] 'process raft request'  (duration: 109.113356ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.539399Z","caller":"traceutil/trace.go:172","msg":"trace[1422317898] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/replicaset-controller; range_end:; response_count:1; response_revision:354; }","duration":"185.985522ms","start":"2025-11-03T18:27:32.353349Z","end":"2025-11-03T18:27:32.539335Z","steps":["trace[1422317898] 'agreement among raft nodes before linearized reading'  (duration: 86.192159ms)","trace[1422317898] 'range keys from in-memory index tree'  (duration: 27.772061ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:32.469596Z","caller":"traceutil/trace.go:172","msg":"trace[983455537] transaction","detail":"{read_only:false; response_revision:355; number_of_response:1; }","duration":"128.15208ms","start":"2025-11-03T18:27:32.341401Z","end":"2025-11-03T18:27:32.469553Z","steps":["trace[983455537] 'process raft request'  (duration: 98.681456ms)","trace[983455537] 'compare'  (duration: 27.763761ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:32.468589Z","caller":"traceutil/trace.go:172","msg":"trace[1656590314] transaction","detail":"{read_only:false; response_revision:357; number_of_response:1; }","duration":"108.63481ms","start":"2025-11-03T18:27:32.359895Z","end":"2025-11-03T18:27:32.468530Z","steps":["trace[1656590314] 'process raft request'  (duration: 108.501997ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.552982Z","caller":"traceutil/trace.go:172","msg":"trace[1408728393] transaction","detail":"{read_only:false; response_revision:358; number_of_response:1; }","duration":"110.730111ms","start":"2025-11-03T18:27:32.442190Z","end":"2025-11-03T18:27:32.552920Z","steps":["trace[1408728393] 'process raft request'  (duration: 109.346778ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.554435Z","caller":"traceutil/trace.go:172","msg":"trace[688308246] transaction","detail":"{read_only:false; response_revision:361; number_of_response:1; }","duration":"109.027747ms","start":"2025-11-03T18:27:32.445357Z","end":"2025-11-03T18:27:32.554385Z","steps":["trace[688308246] 'process raft request'  (duration: 106.742428ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.555205Z","caller":"traceutil/trace.go:172","msg":"trace[2126181081] transaction","detail":"{read_only:false; response_revision:359; number_of_response:1; }","duration":"112.614491ms","start":"2025-11-03T18:27:32.442550Z","end":"2025-11-03T18:27:32.555165Z","steps":["trace[2126181081] 'process raft request'  (duration: 109.236667ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.555283Z","caller":"traceutil/trace.go:172","msg":"trace[623338939] transaction","detail":"{read_only:false; response_revision:362; number_of_response:1; }","duration":"100.955074ms","start":"2025-11-03T18:27:32.454210Z","end":"2025-11-03T18:27:32.555165Z","steps":["trace[623338939] 'process raft request'  (duration: 98.027894ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.555954Z","caller":"traceutil/trace.go:172","msg":"trace[1539962487] transaction","detail":"{read_only:false; response_revision:360; number_of_response:1; }","duration":"112.930021ms","start":"2025-11-03T18:27:32.442987Z","end":"2025-11-03T18:27:32.555917Z","steps":["trace[1539962487] 'process raft request'  (duration: 108.984443ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:32.965073Z","caller":"traceutil/trace.go:172","msg":"trace[2020320528] transaction","detail":"{read_only:false; response_revision:369; number_of_response:1; }","duration":"122.305445ms","start":"2025-11-03T18:27:32.842171Z","end":"2025-11-03T18:27:32.964477Z","steps":["trace[2020320528] 'process raft request'  (duration: 102.652874ms)","trace[2020320528] 'compare'  (duration: 10.695918ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:33.051788Z","caller":"traceutil/trace.go:172","msg":"trace[1302612643] transaction","detail":"{read_only:false; response_revision:372; number_of_response:1; }","duration":"104.802678ms","start":"2025-11-03T18:27:32.946923Z","end":"2025-11-03T18:27:33.051726Z","steps":["trace[1302612643] 'process raft request'  (duration: 104.474847ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:33.351329Z","caller":"traceutil/trace.go:172","msg":"trace[107917532] transaction","detail":"{read_only:false; response_revision:374; number_of_response:1; }","duration":"194.663233ms","start":"2025-11-03T18:27:33.156609Z","end":"2025-11-03T18:27:33.351272Z","steps":["trace[107917532] 'process raft request'  (duration: 183.419963ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:33.357086Z","caller":"traceutil/trace.go:172","msg":"trace[1366400740] transaction","detail":"{read_only:false; response_revision:377; number_of_response:1; }","duration":"103.808884ms","start":"2025-11-03T18:27:33.253223Z","end":"2025-11-03T18:27:33.357032Z","steps":["trace[1366400740] 'process raft request'  (duration: 103.690172ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:33.361862Z","caller":"traceutil/trace.go:172","msg":"trace[292361558] transaction","detail":"{read_only:false; response_revision:375; number_of_response:1; }","duration":"119.670194ms","start":"2025-11-03T18:27:33.242130Z","end":"2025-11-03T18:27:33.361801Z","steps":["trace[292361558] 'process raft request'  (duration: 114.244177ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:33.363285Z","caller":"traceutil/trace.go:172","msg":"trace[1855329073] transaction","detail":"{read_only:false; response_revision:376; number_of_response:1; }","duration":"118.014536ms","start":"2025-11-03T18:27:33.245222Z","end":"2025-11-03T18:27:33.363237Z","steps":["trace[1855329073] 'process raft request'  (duration: 111.43411ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:33.552401Z","caller":"traceutil/trace.go:172","msg":"trace[174704794] transaction","detail":"{read_only:false; response_revision:378; number_of_response:1; }","duration":"112.191281ms","start":"2025-11-03T18:27:33.440158Z","end":"2025-11-03T18:27:33.552349Z","steps":["trace[174704794] 'process raft request'  (duration: 100.758993ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:33.839358Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"176.904542ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/kube-system/kube-proxy\" limit:1 ","response":"range_response_count:1 size:2878"}
{"level":"info","ts":"2025-11-03T18:27:33.839587Z","caller":"traceutil/trace.go:172","msg":"trace[2064360456] range","detail":"{range_begin:/registry/daemonsets/kube-system/kube-proxy; range_end:; response_count:1; response_revision:381; }","duration":"177.169868ms","start":"2025-11-03T18:27:33.662375Z","end":"2025-11-03T18:27:33.839545Z","steps":["trace[2064360456] 'agreement among raft nodes before linearized reading'  (duration: 78.099035ms)","trace[2064360456] 'range keys from in-memory index tree'  (duration: 98.698797ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:33.839703Z","caller":"traceutil/trace.go:172","msg":"trace[237873858] transaction","detail":"{read_only:false; response_revision:382; number_of_response:1; }","duration":"276.210898ms","start":"2025-11-03T18:27:33.563440Z","end":"2025-11-03T18:27:33.839651Z","steps":["trace[237873858] 'process raft request'  (duration: 177.192271ms)","trace[237873858] 'compare'  (duration: 32.594903ms)"],"step_count":2}
{"level":"info","ts":"2025-11-03T18:27:33.841383Z","caller":"traceutil/trace.go:172","msg":"trace[1925271780] transaction","detail":"{read_only:false; response_revision:383; number_of_response:1; }","duration":"177.71982ms","start":"2025-11-03T18:27:33.663627Z","end":"2025-11-03T18:27:33.841347Z","steps":["trace[1925271780] 'process raft request'  (duration: 177.529302ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:34.057537Z","caller":"traceutil/trace.go:172","msg":"trace[1683695696] transaction","detail":"{read_only:false; response_revision:385; number_of_response:1; }","duration":"104.625261ms","start":"2025-11-03T18:27:33.952852Z","end":"2025-11-03T18:27:34.057477Z","steps":["trace[1683695696] 'process raft request'  (duration: 103.04331ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-03T18:27:34.261219Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"102.497759ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/coredns\" limit:1 ","response":"range_response_count:1 size:179"}
{"level":"info","ts":"2025-11-03T18:27:34.261455Z","caller":"traceutil/trace.go:172","msg":"trace[880501341] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/coredns; range_end:; response_count:1; response_revision:385; }","duration":"102.761384ms","start":"2025-11-03T18:27:34.158643Z","end":"2025-11-03T18:27:34.261405Z","steps":["trace[880501341] 'range keys from in-memory index tree'  (duration: 102.09622ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:34.457161Z","caller":"traceutil/trace.go:172","msg":"trace[278067502] transaction","detail":"{read_only:false; response_revision:387; number_of_response:1; }","duration":"108.692349ms","start":"2025-11-03T18:27:34.348413Z","end":"2025-11-03T18:27:34.457105Z","steps":["trace[278067502] 'process raft request'  (duration: 108.061189ms)"],"step_count":1}
{"level":"info","ts":"2025-11-03T18:27:40.940636Z","caller":"traceutil/trace.go:172","msg":"trace[123425872] transaction","detail":"{read_only:false; response_revision:434; number_of_response:1; }","duration":"101.151987ms","start":"2025-11-03T18:27:40.839412Z","end":"2025-11-03T18:27:40.940564Z","steps":["trace[123425872] 'compare'  (duration: 89.513383ms)"],"step_count":1}


==> kernel <==
 19:36:57 up  1:00,  0 users,  load average: 0.91, 0.57, 0.27
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [df76a31cdf05] <==
I1103 18:27:20.369112       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1103 18:27:20.369129       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1103 18:27:20.369342       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1103 18:27:20.369480       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1103 18:27:20.370021       1 repairip.go:210] Starting ipallocator-repair-controller
I1103 18:27:20.370039       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1103 18:27:20.539862       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1103 18:27:20.540787       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1103 18:27:20.546027       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1103 18:27:20.548524       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1103 18:27:20.542235       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1103 18:27:20.556502       1 cache.go:39] Caches are synced for LocalAvailability controller
I1103 18:27:20.556564       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1103 18:27:20.641566       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1103 18:27:20.557862       1 aggregator.go:171] initial CRD sync complete...
I1103 18:27:20.642025       1 autoregister_controller.go:144] Starting autoregister controller
I1103 18:27:20.642055       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1103 18:27:20.642086       1 cache.go:39] Caches are synced for autoregister controller
I1103 18:27:20.642551       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1103 18:27:20.644859       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1103 18:27:20.644905       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1103 18:27:20.739855       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1103 18:27:20.741865       1 policy_source.go:240] refreshing policies
I1103 18:27:20.740445       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1103 18:27:20.743689       1 controller.go:667] quota admission added evaluator for: namespaces
I1103 18:27:20.743893       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
E1103 18:27:20.752929       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
E1103 18:27:20.753214       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1103 18:27:20.851370       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1103 18:27:20.852291       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1103 18:27:21.246743       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1103 18:27:21.249219       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1103 18:27:21.265724       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1103 18:27:21.569037       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1103 18:27:21.674718       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1103 18:27:21.675417       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1103 18:27:25.068735       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1103 18:27:25.219993       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1103 18:27:25.362304       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1103 18:27:25.375195       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1103 18:27:25.376977       1 controller.go:667] quota admission added evaluator for: endpoints
I1103 18:27:25.387593       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1103 18:27:25.458752       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1103 18:27:26.881264       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1103 18:27:27.056434       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1103 18:27:27.169847       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1103 18:27:32.040162       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1103 18:27:32.043419       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1103 18:27:32.245518       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1103 18:27:32.563991       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1103 18:28:22.968140       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:28:30.445291       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:29:36.058586       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:29:55.441868       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:30:55.209523       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:31:07.975517       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:32:20.471321       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:32:21.080515       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:33:27.063560       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 18:33:38.635910       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [e7b37d4d7028] <==
I1103 19:33:17.023268       1 controller.go:119] Starting legacy_token_tracking_controller
I1103 19:33:17.023329       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1103 19:33:17.023466       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1103 19:33:17.023577       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1103 19:33:17.023729       1 local_available_controller.go:156] Starting LocalAvailability controller
I1103 19:33:17.023744       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1103 19:33:17.023770       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1103 19:33:17.023778       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1103 19:33:17.023900       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1103 19:33:17.024352       1 controller.go:142] Starting OpenAPI controller
I1103 19:33:17.024368       1 naming_controller.go:299] Starting NamingConditionController
I1103 19:33:17.024441       1 controller.go:90] Starting OpenAPI V3 controller
I1103 19:33:17.024544       1 establishing_controller.go:81] Starting EstablishingController
I1103 19:33:17.024569       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1103 19:33:17.024586       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1103 19:33:17.024603       1 crd_finalizer.go:269] Starting CRDFinalizer
I1103 19:33:17.023101       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1103 19:33:17.106833       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1103 19:33:17.107022       1 policy_source.go:240] refreshing policies
I1103 19:33:17.186984       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1103 19:33:17.190161       1 aggregator.go:171] initial CRD sync complete...
I1103 19:33:17.194270       1 autoregister_controller.go:144] Starting autoregister controller
I1103 19:33:17.206014       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1103 19:33:17.193988       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1103 19:33:17.200813       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1103 19:33:17.299443       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1103 19:33:17.301371       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1103 19:33:17.490445       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1103 19:33:17.490482       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1103 19:33:17.491625       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1103 19:33:17.504063       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1103 19:33:17.504148       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1103 19:33:17.506775       1 cache.go:39] Caches are synced for autoregister controller
E1103 19:33:17.508204       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1103 19:33:17.585526       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1103 19:33:17.587605       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1103 19:33:17.588258       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1103 19:33:17.588438       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1103 19:33:17.588621       1 cache.go:39] Caches are synced for LocalAvailability controller
I1103 19:33:17.588765       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1103 19:33:17.595465       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1103 19:33:18.028169       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1103 19:33:19.522586       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1103 19:33:21.494172       1 controller.go:667] quota admission added evaluator for: endpoints
I1103 19:33:21.500609       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1103 19:33:21.606124       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1103 19:33:21.616603       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1103 19:33:22.007918       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1103 19:34:16.526866       1 controller.go:667] quota admission added evaluator for: namespaces
I1103 19:34:16.578217       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1103 19:34:16.623809       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1103 19:34:16.709678       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.109.101.6"}
I1103 19:34:16.729112       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.99.9.64"}
I1103 19:34:16.763824       1 controller.go:667] quota admission added evaluator for: jobs.batch
I1103 19:34:18.867901       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 19:34:21.967192       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 19:35:26.680627       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 19:35:35.416614       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 19:36:30.953682       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1103 19:36:50.718720       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [58e9e434f256] <==
I1103 18:27:31.376701       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint"
I1103 18:27:31.442470       1 controllermanager.go:781] "Started controller" controller="daemonset-controller"
I1103 18:27:31.442621       1 daemon_controller.go:310] "Starting daemon sets controller" logger="daemonset-controller"
I1103 18:27:31.442641       1 shared_informer.go:349] "Waiting for caches to sync" controller="daemon sets"
I1103 18:27:31.464176       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1103 18:27:31.550777       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1103 18:27:31.553854       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1103 18:27:31.556968       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1103 18:27:31.557827       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1103 18:27:31.638387       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1103 18:27:31.638611       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1103 18:27:31.641842       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1103 18:27:31.641948       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1103 18:27:31.641965       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1103 18:27:31.642176       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1103 18:27:31.642726       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1103 18:27:31.643584       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1103 18:27:31.643672       1 shared_informer.go:356] "Caches are synced" controller="node"
I1103 18:27:31.644805       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1103 18:27:31.645762       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1103 18:27:31.645803       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1103 18:27:31.646716       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1103 18:27:31.647852       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1103 18:27:31.647939       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1103 18:27:31.648131       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1103 18:27:31.648203       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1103 18:27:31.648217       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1103 18:27:31.648226       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1103 18:27:31.651065       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1103 18:27:31.651351       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1103 18:27:31.651769       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1103 18:27:31.653063       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1103 18:27:31.655227       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1103 18:27:31.655951       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1103 18:27:31.657903       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1103 18:27:31.661743       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1103 18:27:31.662698       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1103 18:27:31.663103       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1103 18:27:31.665878       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1103 18:27:31.666465       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1103 18:27:31.742227       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1103 18:27:31.838904       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1103 18:27:31.742619       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1103 18:27:31.742516       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1103 18:27:31.742679       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1103 18:27:31.742741       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1103 18:27:31.742770       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1103 18:27:31.742791       1 shared_informer.go:356] "Caches are synced" controller="job"
I1103 18:27:31.742858       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1103 18:27:31.841889       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1103 18:27:31.842078       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1103 18:27:31.842247       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1103 18:27:31.742702       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1103 18:27:31.742932       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1103 18:27:31.742958       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1103 18:27:31.742903       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1103 18:27:31.938780       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1103 18:27:31.939182       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1103 18:27:31.939324       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1103 18:27:31.951887       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [cad9e7aeec95] <==
I1103 19:33:20.756716       1 shared_informer.go:349] "Waiting for caches to sync" controller="namespace"
I1103 19:33:20.799147       1 controllermanager.go:781] "Started controller" controller="certificatesigningrequest-approving-controller"
I1103 19:33:20.799249       1 controllermanager.go:739] "Skipping a cloud provider controller" controller="node-route-controller"
I1103 19:33:20.799190       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
I1103 19:33:20.799589       1 shared_informer.go:349] "Waiting for caches to sync" controller="certificate-csrapproving"
I1103 19:33:20.827195       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1103 19:33:20.897103       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1103 19:33:20.900039       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1103 19:33:20.903591       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1103 19:33:20.986049       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1103 19:33:20.987619       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1103 19:33:20.988212       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1103 19:33:20.989709       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1103 19:33:20.989845       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1103 19:33:20.990437       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1103 19:33:20.990726       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1103 19:33:20.990951       1 shared_informer.go:356] "Caches are synced" controller="node"
I1103 19:33:20.991459       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1103 19:33:20.991697       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1103 19:33:20.991724       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1103 19:33:20.991761       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1103 19:33:20.992097       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1103 19:33:20.993867       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1103 19:33:21.002064       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1103 19:33:21.011467       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1103 19:33:21.011677       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1103 19:33:21.012089       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1103 19:33:21.012089       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1103 19:33:21.012139       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1103 19:33:21.012487       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1103 19:33:21.012547       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1103 19:33:21.012571       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1103 19:33:21.088796       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1103 19:33:21.088944       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1103 19:33:21.089868       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1103 19:33:21.090310       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1103 19:33:21.013068       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1103 19:33:21.013559       1 shared_informer.go:356] "Caches are synced" controller="job"
I1103 19:33:21.013043       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1103 19:33:21.013126       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1103 19:33:21.086702       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1103 19:33:21.012793       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1103 19:33:21.013772       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1103 19:33:21.090342       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1103 19:33:21.094699       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1103 19:33:21.095595       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1103 19:33:21.096862       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1103 19:33:21.100098       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1103 19:33:21.106738       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1103 19:33:21.028083       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1103 19:33:21.105062       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1103 19:33:21.102498       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1103 19:33:21.103054       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1103 19:33:21.188915       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1103 19:33:21.203871       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1103 19:33:21.287723       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1103 19:33:21.287816       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1103 19:33:21.287892       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1103 19:33:21.301285       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1103 19:33:21.304211       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [a04348dfc3e5] <==
I1103 19:33:23.667445       1 server_linux.go:53] "Using iptables proxy"
I1103 19:33:23.885934       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1103 19:33:23.987994       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1103 19:33:23.988169       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1103 19:33:23.988350       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1103 19:33:24.124344       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1103 19:33:24.124495       1 server_linux.go:132] "Using iptables Proxier"
I1103 19:33:24.142503       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1103 19:33:24.156881       1 server.go:527] "Version info" version="v1.34.0"
I1103 19:33:24.156983       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1103 19:33:24.160770       1 config.go:200] "Starting service config controller"
I1103 19:33:24.160823       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1103 19:33:24.160880       1 config.go:106] "Starting endpoint slice config controller"
I1103 19:33:24.160896       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1103 19:33:24.160898       1 config.go:309] "Starting node config controller"
I1103 19:33:24.160969       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1103 19:33:24.160944       1 config.go:403] "Starting serviceCIDR config controller"
I1103 19:33:24.161155       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1103 19:33:24.261084       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1103 19:33:24.261158       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1103 19:33:24.261100       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1103 19:33:24.261515       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [cedeee137432] <==
I1103 18:27:39.055897       1 server_linux.go:53] "Using iptables proxy"
I1103 18:27:39.641049       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1103 18:27:39.742000       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1103 18:27:39.742090       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1103 18:27:39.742376       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1103 18:27:39.949808       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1103 18:27:39.950337       1 server_linux.go:132] "Using iptables Proxier"
I1103 18:27:39.990836       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1103 18:27:40.039320       1 server.go:527] "Version info" version="v1.34.0"
I1103 18:27:40.039514       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1103 18:27:40.047836       1 config.go:200] "Starting service config controller"
I1103 18:27:40.049887       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1103 18:27:40.048162       1 config.go:106] "Starting endpoint slice config controller"
I1103 18:27:40.049994       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1103 18:27:40.050112       1 config.go:403] "Starting serviceCIDR config controller"
I1103 18:27:40.050145       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1103 18:27:40.051253       1 config.go:309] "Starting node config controller"
I1103 18:27:40.051350       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1103 18:27:40.051385       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1103 18:27:40.150552       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1103 18:27:40.150815       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1103 18:27:40.150997       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [29fddeaece40] <==
I1103 18:27:14.001457       1 serving.go:386] Generated self-signed cert in-memory
W1103 18:27:20.455352       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1103 18:27:20.455432       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1103 18:27:20.455460       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1103 18:27:20.455479       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1103 18:27:20.859922       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1103 18:27:20.860076       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1103 18:27:20.951950       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1103 18:27:20.952192       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1103 18:27:20.957894       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1103 18:27:20.959994       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1103 18:27:20.964824       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1103 18:27:20.965522       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1103 18:27:21.052277       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1103 18:27:21.058785       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1103 18:27:21.059840       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1103 18:27:21.060212       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1103 18:27:21.140969       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1103 18:27:21.141402       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1103 18:27:21.142339       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1103 18:27:21.143449       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1103 18:27:21.145519       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1103 18:27:21.148831       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1103 18:27:21.143498       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1103 18:27:21.152370       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1103 18:27:21.153802       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1103 18:27:21.154037       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1103 18:27:21.155241       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1103 18:27:21.155829       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1103 18:27:21.156100       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1103 18:27:22.043117       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1103 18:27:22.073813       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1103 18:27:22.143993       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1103 18:27:22.143958       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1103 18:27:22.148104       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1103 18:27:22.244847       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1103 18:27:22.268974       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1103 18:27:22.366889       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1103 18:27:22.367465       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1103 18:27:22.441277       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1103 18:27:22.445830       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1103 18:27:22.448088       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1103 18:27:22.549614       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1103 18:27:22.549614       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1103 18:27:22.559126       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1103 18:27:22.659907       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1103 18:27:22.739408       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1103 18:27:22.745301       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1103 18:27:22.845024       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1103 18:27:24.260318       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1103 18:27:24.373874       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1103 18:27:24.465563       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I1103 18:27:25.352839       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [af236b00bfec] <==
I1103 19:33:15.508580       1 serving.go:386] Generated self-signed cert in-memory
I1103 19:33:17.695077       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1103 19:33:17.695191       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1103 19:33:17.714767       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I1103 19:33:17.714815       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I1103 19:33:17.714775       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1103 19:33:17.714849       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1103 19:33:17.714791       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1103 19:33:17.715117       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1103 19:33:17.715967       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1103 19:33:17.716573       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1103 19:33:17.886125       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1103 19:33:17.886112       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1103 19:33:17.886264       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"


==> kubelet <==
Nov 03 19:33:19 minikube kubelet[1357]: E1103 19:33:19.514904    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:19 minikube kubelet[1357]: E1103 19:33:19.615997    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:19 minikube kubelet[1357]: E1103 19:33:19.717098    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:19 minikube kubelet[1357]: E1103 19:33:19.817343    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:19 minikube kubelet[1357]: E1103 19:33:19.918409    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.019721    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.120145    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.220956    1357 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"minikube\" not found"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.327305    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.342238    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.342314    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.351588    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.351642    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.358915    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.358984    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.369382    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.394677    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.401558    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.488801    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.493871    1357 apiserver.go:52] "Watching apiserver"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.497532    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.526699    1357 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.621587    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.621763    1357 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.624294    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/67906f7e-14ad-48b5-ba00-183fea7bf701-xtables-lock\") pod \"kube-proxy-gm5s6\" (UID: \"67906f7e-14ad-48b5-ba00-183fea7bf701\") " pod="kube-system/kube-proxy-gm5s6"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.624464    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/67906f7e-14ad-48b5-ba00-183fea7bf701-lib-modules\") pod \"kube-proxy-gm5s6\" (UID: \"67906f7e-14ad-48b5-ba00-183fea7bf701\") " pod="kube-system/kube-proxy-gm5s6"
Nov 03 19:33:20 minikube kubelet[1357]: I1103 19:33:20.624560    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/b29353e0-b85d-422f-a8ab-fce4199dd4fa-tmp\") pod \"storage-provisioner\" (UID: \"b29353e0-b85d-422f-a8ab-fce4199dd4fa\") " pod="kube-system/storage-provisioner"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.632885    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 03 19:33:20 minikube kubelet[1357]: E1103 19:33:20.633924    1357 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Nov 03 19:33:44 minikube kubelet[1357]: I1103 19:33:44.831065    1357 scope.go:117] "RemoveContainer" containerID="43a9e4fa7173d93547f32c6fa6688d2d70b1db07c0978cdccd66e7af077a0bf4"
Nov 03 19:33:44 minikube kubelet[1357]: I1103 19:33:44.831525    1357 scope.go:117] "RemoveContainer" containerID="3574593408c1ebbe625f60a28e3de2d2e926e6b72637d2442ab77c11c3606b86"
Nov 03 19:33:44 minikube kubelet[1357]: E1103 19:33:44.831706    1357 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(b29353e0-b85d-422f-a8ab-fce4199dd4fa)\"" pod="kube-system/storage-provisioner" podUID="b29353e0-b85d-422f-a8ab-fce4199dd4fa"
Nov 03 19:33:58 minikube kubelet[1357]: I1103 19:33:58.722763    1357 scope.go:117] "RemoveContainer" containerID="3574593408c1ebbe625f60a28e3de2d2e926e6b72637d2442ab77c11c3606b86"
Nov 03 19:34:16 minikube kubelet[1357]: I1103 19:34:16.890288    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert\") pod \"ingress-nginx-controller-9cc49f96f-5g62x\" (UID: \"4caa3a3a-7f12-4f62-b7da-8136bd21d59d\") " pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-5g62x"
Nov 03 19:34:16 minikube kubelet[1357]: I1103 19:34:16.890554    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zv7rf\" (UniqueName: \"kubernetes.io/projected/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-kube-api-access-zv7rf\") pod \"ingress-nginx-controller-9cc49f96f-5g62x\" (UID: \"4caa3a3a-7f12-4f62-b7da-8136bd21d59d\") " pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-5g62x"
Nov 03 19:34:16 minikube kubelet[1357]: E1103 19:34:16.993913    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:16 minikube kubelet[1357]: E1103 19:34:16.994453    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:17.494216136 +0000 UTC m=+75.453576020 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:17 minikube kubelet[1357]: I1103 19:34:17.094100    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rswps\" (UniqueName: \"kubernetes.io/projected/3c2e5281-be24-41b4-b758-9810c02646a3-kube-api-access-rswps\") pod \"ingress-nginx-admission-create-gcw4r\" (UID: \"3c2e5281-be24-41b4-b758-9810c02646a3\") " pod="ingress-nginx/ingress-nginx-admission-create-gcw4r"
Nov 03 19:34:17 minikube kubelet[1357]: I1103 19:34:17.197126    1357 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7q55t\" (UniqueName: \"kubernetes.io/projected/2cce9b06-3761-4b7c-8612-77e6ec544f12-kube-api-access-7q55t\") pod \"ingress-nginx-admission-patch-krwv8\" (UID: \"2cce9b06-3761-4b7c-8612-77e6ec544f12\") " pod="ingress-nginx/ingress-nginx-admission-patch-krwv8"
Nov 03 19:34:17 minikube kubelet[1357]: E1103 19:34:17.513180    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:17 minikube kubelet[1357]: E1103 19:34:17.513939    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:18.513845347 +0000 UTC m=+76.473205231 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:18 minikube kubelet[1357]: E1103 19:34:18.522326    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:18 minikube kubelet[1357]: E1103 19:34:18.522532    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:20.522507713 +0000 UTC m=+78.481867597 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:20 minikube kubelet[1357]: E1103 19:34:20.540169    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:20 minikube kubelet[1357]: E1103 19:34:20.540354    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:24.54033742 +0000 UTC m=+82.499697204 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:24 minikube kubelet[1357]: E1103 19:34:24.576435    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:24 minikube kubelet[1357]: E1103 19:34:24.576619    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:32.576603731 +0000 UTC m=+90.535963515 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:32 minikube kubelet[1357]: E1103 19:34:32.650301    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:32 minikube kubelet[1357]: E1103 19:34:32.650537    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:34:48.650483705 +0000 UTC m=+106.609843489 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:48 minikube kubelet[1357]: E1103 19:34:48.713562    1357 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 03 19:34:48 minikube kubelet[1357]: E1103 19:34:48.713774    1357 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert podName:4caa3a3a-7f12-4f62-b7da-8136bd21d59d nodeName:}" failed. No retries permitted until 2025-11-03 19:35:20.713745049 +0000 UTC m=+138.677388807 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4caa3a3a-7f12-4f62-b7da-8136bd21d59d-webhook-cert") pod "ingress-nginx-controller-9cc49f96f-5g62x" (UID: "4caa3a3a-7f12-4f62-b7da-8136bd21d59d") : secret "ingress-nginx-admission" not found
Nov 03 19:34:50 minikube kubelet[1357]: I1103 19:34:50.528917    1357 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rswps\" (UniqueName: \"kubernetes.io/projected/3c2e5281-be24-41b4-b758-9810c02646a3-kube-api-access-rswps\") pod \"3c2e5281-be24-41b4-b758-9810c02646a3\" (UID: \"3c2e5281-be24-41b4-b758-9810c02646a3\") "
Nov 03 19:34:50 minikube kubelet[1357]: I1103 19:34:50.532665    1357 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3c2e5281-be24-41b4-b758-9810c02646a3-kube-api-access-rswps" (OuterVolumeSpecName: "kube-api-access-rswps") pod "3c2e5281-be24-41b4-b758-9810c02646a3" (UID: "3c2e5281-be24-41b4-b758-9810c02646a3"). InnerVolumeSpecName "kube-api-access-rswps". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 03 19:34:50 minikube kubelet[1357]: I1103 19:34:50.630008    1357 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-rswps\" (UniqueName: \"kubernetes.io/projected/3c2e5281-be24-41b4-b758-9810c02646a3-kube-api-access-rswps\") on node \"minikube\" DevicePath \"\""
Nov 03 19:34:51 minikube kubelet[1357]: I1103 19:34:51.112699    1357 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="98b20c54e217f99fe1f5236e4723a88851fd78976a12ff4fc81187db61fb00c4"
Nov 03 19:34:51 minikube kubelet[1357]: I1103 19:34:51.637457    1357 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-7q55t\" (UniqueName: \"kubernetes.io/projected/2cce9b06-3761-4b7c-8612-77e6ec544f12-kube-api-access-7q55t\") pod \"2cce9b06-3761-4b7c-8612-77e6ec544f12\" (UID: \"2cce9b06-3761-4b7c-8612-77e6ec544f12\") "
Nov 03 19:34:51 minikube kubelet[1357]: I1103 19:34:51.643535    1357 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2cce9b06-3761-4b7c-8612-77e6ec544f12-kube-api-access-7q55t" (OuterVolumeSpecName: "kube-api-access-7q55t") pod "2cce9b06-3761-4b7c-8612-77e6ec544f12" (UID: "2cce9b06-3761-4b7c-8612-77e6ec544f12"). InnerVolumeSpecName "kube-api-access-7q55t". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 03 19:34:51 minikube kubelet[1357]: I1103 19:34:51.738904    1357 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-7q55t\" (UniqueName: \"kubernetes.io/projected/2cce9b06-3761-4b7c-8612-77e6ec544f12-kube-api-access-7q55t\") on node \"minikube\" DevicePath \"\""
Nov 03 19:34:52 minikube kubelet[1357]: I1103 19:34:52.141207    1357 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9c0cddb00244d1b4171d2dc7d937b450c5af023acfecd7c24d9dc0d46a5161b1"
Nov 03 19:35:46 minikube kubelet[1357]: I1103 19:35:46.030334    1357 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-9cc49f96f-5g62x" podStartSLOduration=66.304004521 podStartE2EDuration="1m30.030315596s" podCreationTimestamp="2025-11-03 19:34:16 +0000 UTC" firstStartedPulling="2025-11-03 19:35:21.426758005 +0000 UTC m=+139.394735001" lastFinishedPulling="2025-11-03 19:35:45.15306898 +0000 UTC m=+163.121046076" observedRunningTime="2025-11-03 19:35:46.030140879 +0000 UTC m=+163.998117875" watchObservedRunningTime="2025-11-03 19:35:46.030315596 +0000 UTC m=+163.998292592"


==> storage-provisioner [1ad45c6324a2] <==
W1103 19:35:57.759533       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:35:57.769101       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:35:59.772300       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:35:59.779112       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:01.784095       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:01.791254       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:03.795900       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:03.801917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:05.806448       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:05.815359       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:07.822487       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:07.828378       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:09.833122       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:09.839308       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:11.842551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:11.847932       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:13.851801       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:13.860943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:15.867104       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:15.876598       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:17.882170       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:17.893312       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:19.897053       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:19.903762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:21.908972       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:21.920238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:23.923816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:23.931365       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:25.936046       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:25.944131       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:27.948442       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:27.954403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:29.957929       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:29.965848       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:31.975102       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:31.987865       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:33.994411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:34.007343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:36.014175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:36.025253       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:38.030141       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:38.039374       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:40.043399       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:40.050481       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:42.054378       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:42.061241       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:44.071383       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:44.085558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:46.093296       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:46.102925       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:48.103713       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:48.112551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:50.116457       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:50.124549       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:52.129495       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:52.138907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:54.146186       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:54.157121       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:56.162285       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1103 19:36:56.169901       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [3574593408c1] <==
I1103 19:33:23.385291       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1103 19:33:44.430227       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

